{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Summer School Assignment 4\n",
    "\n",
    "In this assignment, you guys will be implementing Kmeans clustering from scratch without the use of any machine learning libraries like sklearn for the task of document clustering.\n",
    "\n",
    "The assignment features clustering sysnopses of Top 100 movies as rated by IMdB. The task is to identify movies that fall into the same genre using an unsupervised approach such as K-Means clustering to cluster movies whose synopses refer to similar structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "These libraries will be needed to complete this task. While you'll have most of them already installed if you have followed the previous assignments, you can easily install the rest by googling (:D) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from sklearn import feature_extraction\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "The ratings are taken from IMdB and synopses scraped from IMdB and Wikipedia (we have already done this for you) and stored as pickled lists, you can always take a look at them to get a better feel of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=pickle.load(open('data/titles.pkl','rb'))\n",
    "links=pickle.load(open('data/links.pkl','rb'))\n",
    "genres=pickle.load(open('data/genres.pkl','rb'))\n",
    "ranks=pickle.load(open('data/ranks.pkl','rb'))\n",
    "synopses=pickle.load(open('data/synopses.pkl','rb'))\n",
    "print(len(titles),len(links),len(genres),len(ranks),len(synopses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heads Up!\n",
    "Before you move forward, you'll have to download 3 NLTK packages, namely, `stopwords`, `punkt` and `wordnet` in order to do the required preprocessing of synopses.\n",
    "To download them, just run the cell below, press 'd', type the names of the packages and you're good to go! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 -  Preprocessing the Data\n",
    "You'll have to preprocess the data in this task. Mainly, we'll be removing stopwords, tokenizing and lemmatizing.\n",
    "\n",
    "1. You already know about stopwords from your previous assignments. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. You can read more about them [here](https://en.wikipedia.org/wiki/Stop_words)\n",
    "\n",
    "2. Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. More [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) and [here](https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization). We will be using the WordNet Lemmatizer.\n",
    "\n",
    "3. Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokenization plays a large part in the process of lexical analysis. We will be using NLTK's inbuilt tokennizer. More [here](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load english language stopwords from nltk and store it in the list stopwords \n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the Wornet Lemmatizer and create a lemmatizer object for the english language\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task starts here, fill in the function `tokenize_and_lemmatize()`. \n",
    "* The function returns a nested list of lemmatized tokens. You'll have to filter out the tokens which do not contain any letters. Particularly return the list `lemmas`, `ith` entry of which contains the lemmatized tokens of the `ith` sentence in the given synopses text.\n",
    "\n",
    "    Sample Input: `\"COPS is dope af. Jeena yahaan marna yahaan\"`\n",
    "    \n",
    "    Sample Output: `[[\"COPS\", \"is\", \"dope\", 'af'], [\"Jeena\", \"yahaan\", \"marna\", \"yahaan\"]]`\n",
    "    \n",
    "    Sample Input (Probably more helpful :P): `\"Provision owed ear. Said crying meant cement\"`\n",
    "    \n",
    "    Sample Output: `[[\"provid\", \"ow\", \"ear\"], [\"cry\", \"meant\", \"sem\"]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    #START YOUR CODE HERE\n",
    "    # first tokenize by sentence, then by word to ensure that we have sentence wise list of tokens (i.e. a nested list)\n",
    "    \n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #END YOUR CODE HERE\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create `totalvocab_lemmatized` for all the words in the vocabulary (i.e. all the words in all the synopses combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_lemmatized = []\n",
    "for i in synopses:\n",
    "    allwords_lemmatized = tokenize_and_lemmatize(i)\n",
    "    totalvocab_lemmatized.extend(allwords_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Creating the Features\n",
    "In this task you will have to create the tfidf matrix which will we used to cluster the given synopses. You have leart about tfidf features from the previous assignments. More about it [here](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/) and [here](http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/). You can use the `TfidfVectorizer` provided by sklearn.\n",
    "Store the matrix in the variable `tfidf_matrix`.\n",
    "\n",
    "**A bit about the various parameters you can use to get better features and hence better clusters which using the sklearn tfidfVectorizer.**(This is very very important to get good results)\n",
    "* max_df: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
    "* min_df: this could be a proportion (e.g. 0.2) and the term would have to be in at least 20% of the documents to be considered.\n",
    "* ngram_range: this just means I'll look at unigrams, bigrams and trigrams. See [n-grams](http://en.wikipedia.org/wiki/N-gram).\n",
    "\n",
    "Also, use 'english' for the `stop_words` parameter and `tokenize_and_lemmatize`(the function you wrote) for `tokenizer` parameter in the Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Implementing KMeans\n",
    "\n",
    "This is the most important task of this assignment because in this, you will implement your own K-Means. Fill in the function `KMeans` which will return the labels and cluster centers in the tuple `(labels,centers)` for the given feature matrix `X` and `num_clusters`. We also pass the `max_iter` parameter to run KMeans for that many iterations as it sometimes gets stuck on a local minima. You can learn more about Kmeans clustering [here](https://en.wikipedia.org/wiki/K-means_clustering) and [here](https://www.datascience.com/blog/k-means-clustering).\n",
    "\n",
    "Sample Input: `[[2,1], [2,3], [8,1], [8,3]]`, `num_clusters=2`\n",
    "\n",
    "Sample Output: `([0,0,1,1], [[2,2], [8,2]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kmeans(X, num_clusters=8, max_iter=300):\n",
    "    np.random.seed(42)\n",
    "    #START YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #END YOUR CODE HERE\n",
    "    return (labels, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the computed tf-idf matrix using the function written above to obtain the labels and the centers. You can play with different number of centers and maximum iterations to get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time (labels,centers)=Kmeans(tfidf_matrix.todense(), num_clusters=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe `frame` that stores the the clusters labels, names and genres for all the 100 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': labels, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [labels] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie counts for a particular cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "Sort the cluster centers to get the most important terms per cluster and store it in `cluster_names`. Also print them along with the movies in that cluster to get a feel of the clusters you have made with your KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = np.asarray(centers).argsort()[:, ::-1]\n",
    "cluster_names=[]\n",
    "for i in range(0,order_centroids.shape[0]):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    q=\"\"\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        print(' %s' % terms[ind], end=',')\n",
    "        q+=str(terms[ind])\n",
    "        q+=\" \"\n",
    "    cluster_names.append(q)\n",
    "    print()\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data\n",
    "Now we plot the various movie clusters.\n",
    "Basically we scale the multi-dimentional feature vector by applying 2 dimensional PCA. It is a technique used to visualize multi-dimensional plots in 2 dimensions. More about it [here](http://www.apnorton.com/blog/2016/12/19/Visualizing-Multidimensional-Data-in-Python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as sklearnPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = sklearnPCA(n_components=2) #2-dimensional PCA\n",
    "transformed = pd.DataFrame(pca.fit_transform(tfidf_matrix.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['red','blue','green','gray','black','yellow','orange','brown']\n",
    "for i in range(len(cluster_names)):\n",
    "    plt.scatter(transformed[labels==i][0], transformed[labels==i][1], label=cluster_names[i], c=colors[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And you're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
